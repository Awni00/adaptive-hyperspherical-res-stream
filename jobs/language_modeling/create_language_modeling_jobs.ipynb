{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "\n",
    "import os, sys; sys.path.insert(0, os.path.abspath('../..')) # add project root dir to path\n",
    "from fineweb.model import get_experiment_name\n",
    "from utils.utils import AttributeDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"job_scripts\"\n",
    "out_dir = f'.out'\n",
    "\n",
    "time_str = '00-24:00:00'\n",
    "max_time = '00:23:55:00' # 5 minutes less than the time_str; this is the format PL uses\n",
    "\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_gpu = 8\n",
    "mem_per_cpu = 8\n",
    "n_gpus = 1\n",
    "\n",
    "cluster = 'misha'\n",
    "\n",
    "if cluster == 'grace':\n",
    "    gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # for grace\n",
    "# gpus_constraints = \"a40\" #'\"h100|a100\"' # for misha\n",
    "\n",
    "netid = os.getenv('NETID')\n",
    "project_dir = f\"/home/{netid}/project/adaptive-hyperspherical-res-stream/fineweb\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model, train, and data config\n",
    "import yaml\n",
    "configs_dir = f'{project_dir}/configs'\n",
    "base_config_dir = f'{configs_dir}/base_config'\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'model_config.yaml')) as f:\n",
    "    base_model_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'train_config.yaml')) as f:\n",
    "    base_train_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'data_config.yaml')) as f:\n",
    "    base_data_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L, H = 256, 6, 4\n",
    "\n",
    "manual_norm_weights = False\n",
    "\n",
    "model_configs = dict(\n",
    "    model_arch = [\n",
    "        dict(n_layers=L, d_model=D, n_heads=H),\n",
    "        ],\n",
    "\n",
    "    model_type = ['nGPT'], # 'llama'\n",
    "\n",
    "    residual_module_args = [\n",
    "        # None,\n",
    "        dict(residual_module='ResidualAdaptiveSphericalLERP', residual_module_kwargs=dict(interpolation_weight_activation='linear')),\n",
    "\n",
    "        # dict(residual_module='ResidualSphericalLERP'),\n",
    "\n",
    "        # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=True)),\n",
    "        # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=True, n_spheres=H)),\n",
    "        # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=False, n_spheres=H)),\n",
    "        # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=False)),\n",
    "\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "                residual_module_kwargs=dict(single_weight=True, slerp_weight_map='NormLinear', interpolation_weight_activation='linear')),\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            residual_module_kwargs=dict(single_weight=False, slerp_weight_map='NormLinear', interpolation_weight_activation='linear')),\n",
    "\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            residual_module_kwargs=dict(single_weight=True, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            residual_module_kwargs=dict(single_weight=False, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            residual_module_kwargs=dict(single_weight=True, n_spheres=H, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "        dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            residual_module_kwargs=dict(single_weight=False, n_spheres=H, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "wandb_project = 'language-modeling-nGPT'\n",
    "\n",
    "sequence_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jobs 7\n"
     ]
    }
   ],
   "source": [
    "jobs_overwrite_params = []\n",
    "\n",
    "model_config_product = itertools.product(*[[(k, v) for v in vs] for k, vs in model_configs.items()])\n",
    "\n",
    "for model_config_update in model_config_product:\n",
    "    model_config_update = dict(model_config_update)\n",
    "\n",
    "    # copy base configs\n",
    "    job_model_config = copy.deepcopy(base_model_config)\n",
    "    job_train_config = copy.deepcopy(base_train_config)\n",
    "    job_data_config = copy.deepcopy(base_data_config)\n",
    "\n",
    "    # parse model_config_update\n",
    "    model_arch = model_config_update.pop('model_arch')\n",
    "    model_config_update = {**model_config_update, **model_arch}\n",
    "\n",
    "    if model_config_update['model_type'] == 'llama' and model_config_update['residual_module_args'] is not None:\n",
    "        continue\n",
    "    elif model_config_update['model_type'] == 'nGPT' and model_config_update['residual_module_args'] is None:\n",
    "        continue\n",
    "    elif model_config_update['model_type'] == 'nGPT' and model_config_update['residual_module_args'] is not None:\n",
    "        residual_module_args = model_config_update.pop('residual_module_args')\n",
    "        model_config_update = {**model_config_update, **residual_module_args}\n",
    "\n",
    "    model_config_update['manual_norm_weights'] = manual_norm_weights\n",
    "\n",
    "    # update model config\n",
    "    for k, v in model_config_update.items():\n",
    "        job_model_config[k] = v\n",
    "\n",
    "    # parse train_cofig\n",
    "\n",
    "    job_train_config['wandb_config'] = job_train_config['wandb_config'] | dict(wandb_project=wandb_project)\n",
    "\n",
    "    job_train_config['max_time'] = max_time\n",
    "\n",
    "    # update data config\n",
    "    job_data_config['sequence_length'] = sequence_length\n",
    "\n",
    "    job_config = dict(model_config=job_model_config, train_config=job_train_config, data_config=job_data_config)\n",
    "    job_config = AttributeDict(job_config)\n",
    "    jobs_overwrite_params.append(job_config)\n",
    "\n",
    "print('number of jobs', len(jobs_overwrite_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_config(config_upate, out_dir, uid=None):\n",
    "    global base_model_config, base_train_config, base_data_config\n",
    "    model_config, train_config, data_config = tuple(copy.deepcopy(c) for c in (base_model_config, base_train_config, base_data_config))\n",
    "\n",
    "    model_config.update(config_upate.get('model_config', {}))\n",
    "    train_config.update(config_upate.get('train_config', {}))\n",
    "    data_config.update(config_upate.get('data_config', {}))\n",
    "\n",
    "    experiment_name, _ = get_experiment_name(model_config, data_config, train_config)\n",
    "    experiment_name = experiment_name.replace(' ', '')\n",
    "    if uid is not None:\n",
    "        experiment_name = f\"UID{uid}-{experiment_name}\"\n",
    "\n",
    "    mkdir(os.path.join(out_dir, experiment_name))\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/model_config.yaml'), 'w') as f:\n",
    "        yaml.dump(model_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/train_config.yaml'), 'w') as f:\n",
    "        yaml.dump(train_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/data_config.yaml'), 'w') as f:\n",
    "        yaml.dump(data_config.todict(), f)\n",
    "\n",
    "    return model_config, train_config, data_config, experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_script(experiment_name):\n",
    "    filename = f'{job_directory}/{experiment_name}.job'\n",
    "    with open(filename, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={experiment_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{experiment_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-gpu={cpu_per_gpu}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH --constraint={gpus_constraints}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        if cluster == 'grace':\n",
    "            fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        elif cluster == 'misha':\n",
    "            fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        else:\n",
    "            raise ValueError(f\"Cluster {cluster} not supported\")\n",
    "\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate neural_prog\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # run python script\n",
    "        fh.writelines(f\"srun python train.py --config_dir {configs_dir}/{experiment_name}\\n\") # run python script\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: UID0-nGPT-L6H4D256-ResidualAdaptiveSphericalLERP-IWAct-linear-MNW-False\n",
      "Experiment Name: UID1-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-linear-MNW-False\n",
      "Experiment Name: UID2-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-linear-MNW-False\n",
      "Experiment Name: UID3-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-sigmoid-MNW-False\n",
      "Experiment Name: UID4-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-sigmoid-MNW-False\n",
      "Experiment Name: UID5-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-sigmoid-MNW-False\n",
      "Experiment Name: UID6-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-sigmoid-MNW-False\n"
     ]
    }
   ],
   "source": [
    "job_script_files = []\n",
    "\n",
    "for uid, job_params in enumerate(jobs_overwrite_params):\n",
    "    base_model_config, base_train_config, base_data_config, experiment_name = create_job_config(job_params, configs_dir, uid=uid)\n",
    "\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "    job_script = create_job_script(experiment_name)\n",
    "    job_script_files.append(job_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0\n",
      "response: Submitted batch job 138768, return_code=0, job_script=job_scripts/UID0-nGPT-L6H4D256-ResidualAdaptiveSphericalLERP-IWAct-linear-MNW-False.job\n",
      "response: Submitted batch job 138769, return_code=0, job_script=job_scripts/UID1-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-linear-MNW-False.job\n",
      "response: Submitted batch job 138770, return_code=0, job_script=job_scripts/UID2-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-linear-MNW-False.job\n",
      "response: Submitted batch job 138771, return_code=0, job_script=job_scripts/UID3-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-sigmoid-MNW-False.job\n",
      "response: Submitted batch job 138772, return_code=0, job_script=job_scripts/UID4-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-sigmoid-MNW-False.job\n",
      "response: Submitted batch job 138773, return_code=0, job_script=job_scripts/UID5-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-True-SWM-NormLinear-IWAct-sigmoid-MNW-False.job\n",
      "response: Submitted batch job 138774, return_code=0, job_script=job_scripts/UID6-nGPT-L6H4D256-ResidualAdaptiveSphericalSLERP-SW-False-SWM-NormLinear-IWAct-sigmoid-MNW-False.job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wait_time = 0.5 # number of seconds to wait between job submissions\n",
    "n_trials = 1\n",
    "\n",
    "confirm = input(\"Do you want to submit the jobs? (y/n): \")\n",
    "\n",
    "responses = []\n",
    "\n",
    "if confirm == 'y':\n",
    "    for ir in range(n_trials):\n",
    "        print('Trial:', ir)\n",
    "        for job_script in job_script_files:\n",
    "            response = subprocess.run(['sbatch', job_script], capture_output=True)\n",
    "            print(f\"response: {response.stdout.decode('utf-8').strip()}, return_code={response.returncode}, job_script={job_script}\")\n",
    "            responses.append(response)\n",
    "            time.sleep(wait_time)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Not submitting jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any jobs failed to submit\n",
    "for response in responses:\n",
    "    if not response.stdout.decode('utf-8').startswith('Submitted batch job') or response.returncode != 0:\n",
    "        print(f\"Failed to submit job: {response.stdout.decode('utf-8')}\")\n",
    "        print(f\"stderr: {response.stderr.decode('utf-8')}\")\n",
    "        print(f\"Full response: {response}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
