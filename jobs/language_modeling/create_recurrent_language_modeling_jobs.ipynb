{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "\n",
    "import os, sys; sys.path.insert(0, os.path.abspath('../..')) # add project root dir to path\n",
    "from fineweb.model_recurrent import get_experiment_name\n",
    "from utils.utils import AttributeDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"job_scripts/recurrent\"\n",
    "out_dir = f'.out'\n",
    "\n",
    "time_str = '00-24:00:00'\n",
    "max_time = '00:23:55:00' # 5 minutes less than the time_str; this is the format PL uses\n",
    "\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_gpu = 8\n",
    "mem_per_cpu = 8\n",
    "n_gpus = 1\n",
    "\n",
    "cluster = 'misha'\n",
    "\n",
    "if cluster == 'grace':\n",
    "    gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # for grace\n",
    "# gpus_constraints = \"a40\" #'\"h100|a100\"' # for misha\n",
    "\n",
    "netid = os.getenv('NETID')\n",
    "project_dir = f\"/home/{netid}/project/adaptive-hyperspherical-res-stream/fineweb\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model, train, and data config\n",
    "import yaml\n",
    "configs_dir = f'{project_dir}/configs/recurrent'\n",
    "base_config_dir = f'{configs_dir}/base_config'\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'model_config.yaml')) as f:\n",
    "    base_model_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'train_config.yaml')) as f:\n",
    "    base_train_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'data_config.yaml')) as f:\n",
    "    base_data_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L, T, H = 256, 2, 1, 4\n",
    "sequence_length_map = {256: 256, 384: 512, 512: 512} # map from d_model to sequence_length\n",
    "\n",
    "manual_norm_weights = True\n",
    "micro_batch_size = 64\n",
    "\n",
    "# model_types = ['llama', 'nGPT', 'baseline_transformer'] # 'llama', 'nGPT'\n",
    "model_types = ['nGPT'] # 'llama', 'nGPT'\n",
    "\n",
    "model_archs = [\n",
    "        # dict(n_layers=L, d_model=D, n_heads=H, n_iters=T),\n",
    "\n",
    "        dict(d_model=256, n_heads=4, n_layers=1, n_iters=6),\n",
    "        dict(d_model=256, n_heads=4, n_layers=2, n_iters=3),\n",
    "        dict(d_model=256, n_heads=4, n_layers=6, n_iters=1),\n",
    "\n",
    "        dict(d_model=384, n_heads=8, n_layers=1, n_iters=8),\n",
    "        dict(d_model=384, n_heads=8, n_layers=2, n_iters=4),\n",
    "        dict(d_model=384, n_heads=8, n_layers=8, n_iters=1),\n",
    "\n",
    "        dict(d_model=512, n_heads=8, n_layers=1, n_iters=6),\n",
    "        dict(d_model=512, n_heads=8, n_layers=2, n_iters=4),\n",
    "        dict(d_model=512, n_heads=8, n_layers=8, n_iters=1),\n",
    "        ]\n",
    "\n",
    "model_type_configs = dict(\n",
    "    nGPT = dict(\n",
    "        residual_module_args = [\n",
    "            dict(residual_module='ResidualSphericalLERPBase'),\n",
    "\n",
    "            # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=True)),\n",
    "            # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=False)),\n",
    "            # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=True, n_spheres=H)),\n",
    "            # dict(residual_module='ResidualSphericalSLERP', residual_module_kwargs=dict(single_weight=False, n_spheres=H)),\n",
    "\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #         residual_module_kwargs=dict(single_weight=True, slerp_weight_map='NormLinear', interpolation_weight_activation='linear')),\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #     residual_module_kwargs=dict(single_weight=False, slerp_weight_map='NormLinear', interpolation_weight_activation='linear')),\n",
    "\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #     residual_module_kwargs=dict(single_weight=True, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #     residual_module_kwargs=dict(single_weight=False, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #     residual_module_kwargs=dict(single_weight=True, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid', bias=True)),\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "            #     residual_module_kwargs=dict(single_weight=False, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid', bias=True)),\n",
    "\n",
    "            dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "                residual_module_kwargs=dict(single_weight=True, n_spheres=H, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "            # dict(residual_module='ResidualAdaptiveSphericalSLERP',\n",
    "                # residual_module_kwargs=dict(single_weight=False, n_spheres=H, slerp_weight_map='NormLinear', interpolation_weight_activation='sigmoid')),\n",
    "            ],\n",
    "            manual_norm_weights = [manual_norm_weights],\n",
    "            gpt_special_init = [True]\n",
    "        ),\n",
    "\n",
    "    llama = dict(),\n",
    "\n",
    "    baseline_transformer = dict(\n",
    "        norm_config = [dict(norm_method='pre-norm', norm_type='rmsnorm')],\n",
    "        mlp_activation = ['swiglu'],\n",
    "        pos_enc_type = ['rotary'],\n",
    "        bias = [False],\n",
    "        gpt_special_init = [False, True],\n",
    "    ),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "wandb_project = 'recurrent-language-modeling-nGPT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model architecture configs:  9\n",
      "\n",
      "creating jobs for model_type = nGPT\n",
      "number of model_type_configs: 2\n",
      "\n",
      "total number of jobs: 18\n"
     ]
    }
   ],
   "source": [
    "jobs_overwrite_params = []\n",
    "\n",
    "print('number of model architecture configs: ', len(model_archs))\n",
    "\n",
    "for model_type in model_types:\n",
    "    print()\n",
    "    print('creating jobs for model_type =', model_type)\n",
    "\n",
    "    model_type_config_product = list(itertools.product(*[[(k, v) for v in vs] for k, vs in model_type_configs[model_type].items()]))\n",
    "\n",
    "    print('number of model_type_configs:', len(model_type_config_product))\n",
    "\n",
    "    for model_arch_config, model_type_config in itertools.product(model_archs, model_type_config_product):\n",
    "        # copy base configs\n",
    "        job_model_config = copy.deepcopy(base_model_config)\n",
    "        job_train_config = copy.deepcopy(base_train_config)\n",
    "        job_data_config = copy.deepcopy(base_data_config)\n",
    "\n",
    "        # set_model_type\n",
    "        job_model_config['model_type'] = model_type\n",
    "\n",
    "        # update model config\n",
    "        job_model_config.update(model_arch_config) # update with model_architecture\n",
    "\n",
    "        # update kwargs for model_type\n",
    "        model_type_config_update = dict(model_type_config)\n",
    "        if f'{model_type}_kwargs' in job_model_config:\n",
    "            job_model_config[f'{model_type}_kwargs'].update(model_type_config_update)\n",
    "        else:\n",
    "            job_model_config[f'{model_type}_kwargs'] = model_type_config_update\n",
    "\n",
    "        # remove other model_type kwargs\n",
    "        for other_model_type in ['nGPT', 'llama', 'baseline_transformer']:\n",
    "            if other_model_type != model_type and f'{other_model_type}_kwargs' in job_model_config:\n",
    "                del job_model_config[f'{other_model_type}_kwargs']\n",
    "\n",
    "        # parse train_cofig\n",
    "        job_train_config['wandb_config'] = job_train_config['wandb_config'] | dict(wandb_project=wandb_project)\n",
    "\n",
    "        job_train_config['max_time'] = max_time\n",
    "\n",
    "        job_train_config['micro_batch_size'] = micro_batch_size\n",
    "\n",
    "        # set learning rate schedule config\n",
    "        job_train_config['cosine_scheduler_config'] = dict(warmup_steps=0, max_steps=None) # test warmup_steps=0, for normalized models\n",
    "\n",
    "        # update data config\n",
    "        job_data_config['sequence_length'] = sequence_length_map[job_model_config['d_model']]\n",
    "\n",
    "        job_config = dict(model_config=job_model_config, train_config=job_train_config, data_config=job_data_config)\n",
    "        job_config = AttributeDict(job_config)\n",
    "        jobs_overwrite_params.append(job_config)\n",
    "\n",
    "print('\\ntotal number of jobs:', len(jobs_overwrite_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_config(job_configs, out_dir, uid=None):\n",
    "    # global base_model_config, base_train_config, base_data_config\n",
    "    # model_config, train_config, data_config = tuple(copy.deepcopy(c) for c in (base_model_config, base_train_config, base_data_config))\n",
    "\n",
    "    # model_config.update(config_upate.get('model_config', {}))\n",
    "    # train_config.update(config_upate.get('train_config', {}))\n",
    "    # data_config.update(config_upate.get('data_config', {}))\n",
    "    model_config, train_config, data_config = job_configs.model_config, job_configs.train_config, job_configs.data_config\n",
    "\n",
    "    experiment_name, _ = get_experiment_name(model_config, data_config, train_config)\n",
    "    experiment_name = experiment_name.replace(' ', '')\n",
    "    if uid is not None:\n",
    "        experiment_name = f\"UID{uid}-{experiment_name}\"\n",
    "\n",
    "    mkdir(os.path.join(out_dir, experiment_name))\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/model_config.yaml'), 'w') as f:\n",
    "        yaml.dump(model_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/train_config.yaml'), 'w') as f:\n",
    "        yaml.dump(train_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/data_config.yaml'), 'w') as f:\n",
    "        yaml.dump(data_config.todict(), f)\n",
    "\n",
    "    return model_config, train_config, data_config, experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_script(experiment_name):\n",
    "    filename = f'{job_directory}/{experiment_name}.job'\n",
    "    with open(filename, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={experiment_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{experiment_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        if cluster == 'misha':\n",
    "            fh.writelines(f\"#SBATCH --cpus-per-gpu={cpu_per_gpu}\\n\")\n",
    "        else:\n",
    "            fh.writelines(f\"#SBATCH --cpus-per-task={cpu_per_gpu * n_gpus}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        if gpus_constraints is not None:\n",
    "            fh.writelines(f\"#SBATCH --constraint={gpus_constraints}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        if cluster == 'grace':\n",
    "            fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        elif cluster == 'misha':\n",
    "            fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        else:\n",
    "            raise ValueError(f\"Cluster {cluster} not supported\")\n",
    "\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate neural_prog\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # run python script\n",
    "        fh.writelines(f\"srun python train_recurrent.py --config_dir {configs_dir}/{experiment_name}\\n\") # run python script\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: UID0-nGPT-L1T6H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID1-nGPT-L1T6H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID2-nGPT-L2T3H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID3-nGPT-L2T3H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID4-nGPT-L6T1H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID5-nGPT-L6T1H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256\n",
      "Experiment Name: UID6-nGPT-L1T8H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID7-nGPT-L1T8H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID8-nGPT-L2T4H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID9-nGPT-L2T4H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID10-nGPT-L8T1H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID11-nGPT-L8T1H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID12-nGPT-L1T6H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID13-nGPT-L1T6H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID14-nGPT-L2T4H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID15-nGPT-L2T4H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID16-nGPT-L8T1H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n",
      "Experiment Name: UID17-nGPT-L8T1H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512\n"
     ]
    }
   ],
   "source": [
    "job_script_files = []\n",
    "\n",
    "for uid, job_params in enumerate(jobs_overwrite_params):\n",
    "    base_model_config, base_train_config, base_data_config, experiment_name = create_job_config(job_params, configs_dir, uid=uid)\n",
    "\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "    job_script = create_job_script(experiment_name)\n",
    "    job_script_files.append(job_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0\n",
      "response: Submitted batch job 140675, return_code=0, job_script=job_scripts/recurrent/UID0-nGPT-L1T6H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140676, return_code=0, job_script=job_scripts/recurrent/UID1-nGPT-L1T6H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140677, return_code=0, job_script=job_scripts/recurrent/UID2-nGPT-L2T3H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140678, return_code=0, job_script=job_scripts/recurrent/UID3-nGPT-L2T3H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140679, return_code=0, job_script=job_scripts/recurrent/UID4-nGPT-L6T1H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140680, return_code=0, job_script=job_scripts/recurrent/UID5-nGPT-L6T1H4D256-ResidualSphericalLERPBase-MNW-True-GPTInit-True-256.job\n",
      "response: Submitted batch job 140681, return_code=0, job_script=job_scripts/recurrent/UID6-nGPT-L1T8H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140682, return_code=0, job_script=job_scripts/recurrent/UID7-nGPT-L1T8H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140683, return_code=0, job_script=job_scripts/recurrent/UID8-nGPT-L2T4H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140684, return_code=0, job_script=job_scripts/recurrent/UID9-nGPT-L2T4H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140685, return_code=0, job_script=job_scripts/recurrent/UID10-nGPT-L8T1H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140686, return_code=0, job_script=job_scripts/recurrent/UID11-nGPT-L8T1H8D384-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140687, return_code=0, job_script=job_scripts/recurrent/UID12-nGPT-L1T6H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140688, return_code=0, job_script=job_scripts/recurrent/UID13-nGPT-L1T6H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140689, return_code=0, job_script=job_scripts/recurrent/UID14-nGPT-L2T4H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140690, return_code=0, job_script=job_scripts/recurrent/UID15-nGPT-L2T4H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140691, return_code=0, job_script=job_scripts/recurrent/UID16-nGPT-L8T1H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "response: Submitted batch job 140692, return_code=0, job_script=job_scripts/recurrent/UID17-nGPT-L8T1H8D512-ResidualSphericalLERPBase-MNW-True-GPTInit-True-512.job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wait_time = 0.5 # number of seconds to wait between job submissions\n",
    "n_trials = 1\n",
    "\n",
    "confirm = input(\"Do you want to submit the jobs? (y/n): \")\n",
    "\n",
    "responses = []\n",
    "\n",
    "if confirm == 'y':\n",
    "    for ir in range(n_trials):\n",
    "        print('Trial:', ir)\n",
    "        for job_script in job_script_files:\n",
    "            response = subprocess.run(['sbatch', job_script], capture_output=True)\n",
    "            print(f\"response: {response.stdout.decode('utf-8').strip()}, return_code={response.returncode}, job_script={job_script}\")\n",
    "            responses.append(response)\n",
    "            time.sleep(wait_time)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Not submitting jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any jobs failed to submit\n",
    "for response in responses:\n",
    "    if not response.stdout.decode('utf-8').startswith('Submitted batch job') or response.returncode != 0:\n",
    "        print(f\"Failed to submit job: {response.stdout.decode('utf-8')}\")\n",
    "        print(f\"stderr: {response.stderr.decode('utf-8')}\")\n",
    "        print(f\"Full response: {response}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
